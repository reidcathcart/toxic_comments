{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:13:50.851565Z",
     "start_time": "2018-02-06T01:13:50.026777Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling as ppf\n",
    "import datetime\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T02:18:33.672133Z",
     "start_time": "2018-02-06T02:18:33.663116Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split,GridSearchCV,cross_val_score,KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:13:51.514748Z",
     "start_time": "2018-02-06T01:13:51.488732Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "s = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def text_process(comment_column):\n",
    "    filtered_rows = []\n",
    "    for rows in comment_column:\n",
    "        only_words = tokenizer.tokenize(rows)\n",
    "        no_stopwords = [word for word in only_words if word.lower() not in stopwords_set]\n",
    "        #stems = [s.stem(word) for word in no_stopwords]\n",
    "        filtered_rows.append(' '.join(no_stopwords))\n",
    "    return filtered_rows\n",
    "\n",
    "def find_badwords(comment_column):\n",
    "    filtered_rows = []\n",
    "    for rows in comment_column:\n",
    "        only_words = tokenizer.tokenize(rows)\n",
    "        bad = [word for word in only_words if word.lower() in badwords_set]\n",
    "        total_bad,=np.shape(bad)\n",
    "        filtered_rows.append(total_bad)\n",
    "    return filtered_rows\n",
    "\n",
    "def get_tags(df):\n",
    "    tags=[]\n",
    "    alltags=[]\n",
    "    rows = df.shape[0]\n",
    "    for r in range(df.index.min(),df.index.max()+1):\n",
    "        for f in df.columns:\n",
    "            if df.loc[r,f] == 1:\n",
    "                tags.append(str(f))\n",
    "        alltags.append(tags)\n",
    "        tags=[]\n",
    "    return alltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:13:54.314953Z",
     "start_time": "2018-02-06T01:13:51.516751Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_initial = pd.read_csv('train.csv.zip')\n",
    "df_test_initial = pd.read_csv('test.csv.zip')\n",
    "df_sub = pd.read_csv('sample_submission.csv.zip')\n",
    "df_features=pd.concat([df_train_initial,df_test_initial]).reset_index(drop=True)\n",
    "df_features['source'] = ''\n",
    "df_features.loc[0:len(df_train_initial),['source']] = 'train'\n",
    "df_features.loc[len(df_train_initial):,['source']] = 'test'\n",
    "\n",
    "initialcols = list(df_train_initial.columns[df_train_initial.dtypes == 'int64'])\n",
    "\n",
    "punct_set = set(string.punctuation)\n",
    "\n",
    "badwords = pd.read_csv('badwords.txt',header=None)\n",
    "badwords.rename(columns={0:'badwords'},inplace=True)\n",
    "badwords['badwords'] = badwords['badwords'].str.lower()\n",
    "badwords_set = set(badwords['badwords'].str.replace('*',''))\n",
    "\n",
    "hatewords = pd.read_csv('hatewords.txt',header=None)\n",
    "hatewords.rename(columns={0:'hatewords'},inplace=True)\n",
    "hatewords['hatewords'] = hatewords['hatewords'].str.lower()\n",
    "hatewords_set = set(hatewords['hatewords'].str.replace('*',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nulls in data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate whether a row is 'clean' and how many flags it has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:13:54.759214Z",
     "start_time": "2018-02-06T01:13:54.316441Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features['clean'] = df_features.loc[:,initialcols].sum(axis=1).apply(lambda x: 1 if x==0 else 0)\n",
    "df_features['total_flags'] = df_features.loc[:,initialcols].sum(axis=1)\n",
    "df_features['comment_length'] = df_features.comment_text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:14:49.560281Z",
     "start_time": "2018-02-06T01:13:54.760215Z"
    }
   },
   "outputs": [],
   "source": [
    "word_count = []\n",
    "comments_processed = text_process(df_features.comment_text)\n",
    "word_count_proc = []\n",
    "comment_len_proc = []\n",
    "words_all_caps = []\n",
    "punct_count = []\n",
    "badword_count = []\n",
    "badword_allcaps = []\n",
    "most_common_word_count = []\n",
    "mcw_is_badword = []\n",
    "hateword_count = []\n",
    "\n",
    "for row in df_features.comment_text:\n",
    "    word_count.append(len(row.split()))\n",
    "    punct_count.append(sum([1 for x in row if x in punct_set]))\n",
    "    badword_count.append(sum([1 for x in row.lower().split() if x in badwords_set]))\n",
    "    hateword_count.append(sum([1 for x in row.lower().split() if x in hatewords_set]))\n",
    "    \n",
    "for row in comments_processed:\n",
    "    word_count_proc.append(len(row.split()))\n",
    "    comment_len_proc.append(len(row))\n",
    "    words_all_caps.append(np.sum([x.isupper() for x in row.split() if len(x)>1]))\n",
    "    badword_allcaps.append(np.sum([x.isupper() for x in row.split() if (len(x)>1)&(x.lower() in badwords_set)]))\n",
    "    q=Counter(tokenizer.tokenize(row))\n",
    "    if len(q)>0:\n",
    "        most_common_word_count.append(q.most_common(1)[0][1])\n",
    "        mcw_is_badword.append(q.most_common(1)[0][0].lower() in badwords_set)\n",
    "    else:\n",
    "        most_common_word_count.append(0)\n",
    "        mcw_is_badword.append(0)    \n",
    "\n",
    "df_features['comment_word_all_caps_processed'] = words_all_caps\n",
    "df_features['comment_word_count_processed'] = word_count_proc\n",
    "df_features['comment_length_processed'] = comment_len_proc\n",
    "df_features['comment_word_count'] = word_count\n",
    "df_features['comment_punct_count'] = punct_count\n",
    "df_features['comment_badword_count'] = badword_count\n",
    "df_features['comment_badword_all_caps_count'] = badword_allcaps\n",
    "df_features['comment_most_common_word_count'] = most_common_word_count\n",
    "df_features['comment_mcw_is_badword'] = mcw_is_badword\n",
    "df_features['comment_mcw_is_badword'] = df_features.comment_mcw_is_badword.astype(bool)\n",
    "df_features['comment_hateword_count'] = hateword_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:14:49.651388Z",
     "start_time": "2018-02-06T01:14:49.562283Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features['mean_word_size_processed'] = df_features.comment_word_count_processed / df_features.comment_length_processed\n",
    "df_features['mean_word_size'] = df_features.comment_word_count / df_features.comment_length\n",
    "df_features['punct_pct'] = df_features.comment_punct_count / df_features.comment_length * 100\n",
    "df_features['badword_pct'] = df_features.comment_badword_count / df_features.comment_word_count_processed * 100\n",
    "df_features['hateword_pct'] = df_features.comment_hateword_count / df_features.comment_word_count_processed * 100\n",
    "df_features['all_caps_pct'] = df_features.comment_word_all_caps_processed / df_features.comment_word_count_processed * 100\n",
    "df_features['all_caps_pct_badwords'] = df_features.comment_badword_all_caps_count / df_features.comment_word_all_caps_processed * 100\n",
    "df_features['mcw_pct'] = df_features.comment_most_common_word_count/df_features.comment_word_count_processed*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:14:50.085191Z",
     "start_time": "2018-02-06T01:14:49.652894Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_features[df_features.source == 'train'].copy()\n",
    "y = df_train_initial[initialcols]\n",
    "df_train.drop(initialcols,inplace=True,axis=1)\n",
    "df_train.drop(['comment_text','id','source'],inplace=True,axis=1)\n",
    "df_train.fillna(0,inplace=True)\n",
    "df_test = df_features[df_features.source == 'test'].copy()\n",
    "df_test.fillna(0,inplace=True)\n",
    "df_test.drop(initialcols,inplace=True,axis=1)\n",
    "df_test.drop(['comment_text','id','source'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:14:50.091198Z",
     "start_time": "2018-02-06T01:14:50.086697Z"
    }
   },
   "outputs": [],
   "source": [
    "# param_test1 = {\n",
    "#     'learning_rate': [i/10.0 for i in range(0,5)],\n",
    "#     'gamma': [i/10.0 for i in range(0,5)],\n",
    "#     'max_depth':range(3,10,2),\n",
    "#     'min_child_weight':range(1,6,2)\n",
    "# }\n",
    "# {'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3}\n",
    "# gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(n_estimators=100, subsample=0.8, colsample_bytree=0.8, \n",
    "#                                                   objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "#                                                   param_grid = param_test1, scoring='neg_log_loss', cv=5)\n",
    "# gsearch1.fit(X_train,y_train)\n",
    "# gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:19:37.932448Z",
     "start_time": "2018-02-06T01:19:37.922442Z"
    }
   },
   "outputs": [],
   "source": [
    "pars= {'n_estimators':100, 'subsample':0.8, 'colsample_bytree':0.8,  \n",
    "                  'objective': 'binary:logistic', 'nthread':4, 'scale_pos_weight':1, 'seed':27, \n",
    "                  'gamma':0.1, 'learning_rate':0.1, 'max_depth':3, 'min_child_weight': 3}\n",
    "kfold = 5\n",
    "\n",
    "model_preds = {}\n",
    "model = {}\n",
    "model_final = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T01:55:13.464243Z",
     "start_time": "2018-02-06T01:55:13.426227Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_test_model(flag,pars,kfold,df_train,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_train, y[flag].values, test_size=0.33, random_state=42)\n",
    "\n",
    "    model_preds[flag] = pd.DataFrame(y_test,columns=['true'],index=X_test.index)\n",
    "    model_preds[flag][flag] = 0\n",
    "\n",
    "    model[flag] = xgb.XGBClassifier()\n",
    "    model[flag].set_params(**pars)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=kfold, random_state=42)\n",
    "\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_valid = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_fold, y_valid = y_train[train_index], y_train[test_index]\n",
    "        model[flag].fit(X_train_fold,y_train_fold)\n",
    "        preds = model[flag].predict_proba(X_test)[:,1]\n",
    "        model_preds[flag][flag] += preds/kfold\n",
    "\n",
    "    print(flag,' model predictions:\\n',log_loss(model_preds[flag]['true'],model_preds[flag][flag]))\n",
    "\n",
    "def make_model(initialcols,pars,df_train,y,df_test):\n",
    "\n",
    "    for flag in initialcols:\n",
    "        model_final[flag] = xgb.XGBClassifier()\n",
    "        model_final[flag].set_params(**pars)\n",
    "\n",
    "        model_final[flag].fit(df_train,y[flag])\n",
    "        preds = model_final[flag].predict_proba(df_test)[:,1]\n",
    "        df_sub[flag] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T02:19:14.174518Z",
     "start_time": "2018-02-06T02:19:13.352939Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_train_scaled = scaler.fit_transform(df_train)\n",
    "df_train_scaled = pd.DataFrame(df_train_scaled,index=df_train.index,columns=df_train.columns)\n",
    "scaler = MinMaxScaler()\n",
    "df_test_scaled = scaler.fit_transform(df_test)\n",
    "df_test_scaled = pd.DataFrame(df_test_scaled,index=df_test.index,columns=df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T02:20:22.184429Z",
     "start_time": "2018-02-06T02:19:25.436027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic  model predictions:\n",
      " 0.0196839104204\n",
      "severe_toxic  model predictions:\n",
      " 0.0133197676631\n",
      "threat  model predictions:\n",
      " 0.00935788848831\n",
      "obscene  model predictions:\n",
      " 0.0238796323996\n",
      "insult  model predictions:\n",
      " 0.0270975334964\n",
      "identity_hate  model predictions:\n",
      " 0.0170791516161\n"
     ]
    }
   ],
   "source": [
    "make_test_model('toxic',pars,kfold,df_train_scaled,y)\n",
    "make_test_model('severe_toxic',pars,kfold,df_train_scaled,y)\n",
    "make_test_model('threat',pars,kfold,df_train_scaled,y)\n",
    "make_test_model('obscene',pars,kfold,df_train_scaled,y)\n",
    "make_test_model('insult',pars,kfold,df_train_scaled,y)\n",
    "make_test_model('identity_hate',pars,kfold,df_train_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T02:21:01.069107Z",
     "start_time": "2018-02-06T02:20:35.715298Z"
    }
   },
   "outputs": [],
   "source": [
    "make_model(initialcols,pars,df_train_scaled,y,df_test_scaled)\n",
    "df_sub.to_csv('df_sub_'+datetime.datetime.now().strftime('%Y%m%d%I%M')+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T02:24:25.505671Z",
     "start_time": "2018-02-06T02:24:25.349562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000124</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000129</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16439.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000131</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000132</th>\n",
       "      <td>NaN</td>\n",
       "      <td>255.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000133</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000134</th>\n",
       "      <td>NaN</td>\n",
       "      <td>190.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000135</th>\n",
       "      <td>NaN</td>\n",
       "      <td>743.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000137</th>\n",
       "      <td>NaN</td>\n",
       "      <td>317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000138</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000139</th>\n",
       "      <td>NaN</td>\n",
       "      <td>133.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000140</th>\n",
       "      <td>NaN</td>\n",
       "      <td>55683.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000142</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000143</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10421.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000144</th>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>309.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000146</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2843.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000150</th>\n",
       "      <td>NaN</td>\n",
       "      <td>439.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000151</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5075.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000152</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000153</th>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000154</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000155</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2732.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000156</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2627.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000157</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000158</th>\n",
       "      <td>NaN</td>\n",
       "      <td>272.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000160</th>\n",
       "      <td>NaN</td>\n",
       "      <td>123.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.000161</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001567</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001590</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001596</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001598</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001605</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001632</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001641</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001643</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001653</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001655</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001675</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001709</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001714</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>272.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001736</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001737</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001763</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001789</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001804</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001852</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001864</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001903</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001904</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001928</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.002025</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.002039</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.002079</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.002129</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.002170</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.002300</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>923 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0.000124    NaN           1.0      NaN     NaN     NaN            NaN\n",
       "0.000129    NaN       16439.0      NaN     NaN     NaN            NaN\n",
       "0.000131    NaN           2.0      NaN     NaN     NaN            NaN\n",
       "0.000132    NaN         255.0      NaN     NaN     NaN            NaN\n",
       "0.000133    NaN           1.0      NaN     NaN     NaN            NaN\n",
       "0.000134    NaN         190.0      NaN     NaN     NaN            NaN\n",
       "0.000135    NaN         743.0      NaN     NaN     NaN            NaN\n",
       "0.000137    NaN         317.0      NaN     NaN     NaN            NaN\n",
       "0.000138    NaN           4.0      NaN     NaN     NaN            NaN\n",
       "0.000139    NaN         133.0      NaN     NaN     NaN            NaN\n",
       "0.000140    NaN       55683.0      NaN     NaN     NaN            NaN\n",
       "0.000142    NaN        1054.0      NaN     NaN     NaN            NaN\n",
       "0.000143    NaN       10421.0      NaN     NaN     NaN            NaN\n",
       "0.000144    NaN          82.0      NaN     NaN     NaN            NaN\n",
       "0.000145    NaN         309.0      NaN     NaN     NaN            NaN\n",
       "0.000146    NaN        1354.0      NaN     NaN     NaN            NaN\n",
       "0.000148    NaN           5.0      NaN     NaN     NaN            NaN\n",
       "0.000149    NaN        2843.0      NaN     NaN     NaN            NaN\n",
       "0.000150    NaN         439.0      NaN     NaN     NaN            NaN\n",
       "0.000151    NaN        5075.0      NaN     NaN     NaN            NaN\n",
       "0.000152    NaN        8120.0      NaN     NaN     NaN            NaN\n",
       "0.000153    NaN          81.0      NaN     NaN     NaN            NaN\n",
       "0.000154    NaN        1081.0      NaN     NaN     NaN            NaN\n",
       "0.000155    NaN        2732.0      NaN     NaN     NaN            NaN\n",
       "0.000156    NaN        2627.0      NaN     NaN     NaN            NaN\n",
       "0.000157    NaN          51.0      NaN     NaN     NaN            NaN\n",
       "0.000158    NaN         272.0      NaN     NaN     NaN            NaN\n",
       "0.000159    NaN          33.0      NaN     NaN     NaN            NaN\n",
       "0.000160    NaN         123.0      NaN     NaN     NaN            NaN\n",
       "0.000161    NaN          29.0      NaN     NaN     NaN            NaN\n",
       "...         ...           ...      ...     ...     ...            ...\n",
       "0.001567    NaN           1.0      NaN     9.0     NaN            NaN\n",
       "0.001590    NaN           1.0      NaN    14.0     NaN            NaN\n",
       "0.001596    NaN           1.0      NaN    36.0     NaN            NaN\n",
       "0.001598    NaN           2.0      NaN     8.0     NaN            NaN\n",
       "0.001605    NaN           5.0      NaN    11.0     NaN            NaN\n",
       "0.001632    NaN           1.0      NaN   181.0     NaN            NaN\n",
       "0.001641    NaN           1.0      NaN    57.0     NaN            NaN\n",
       "0.001643    NaN           1.0      NaN    47.0     NaN            NaN\n",
       "0.001653    NaN           1.0      NaN    17.0     NaN            NaN\n",
       "0.001655    NaN           1.0      NaN    84.0     NaN            NaN\n",
       "0.001675    NaN           1.0      NaN    21.0     NaN            NaN\n",
       "0.001709    NaN           1.0      NaN     2.0     NaN            1.0\n",
       "0.001714    NaN           1.0      NaN   272.0     NaN            NaN\n",
       "0.001736    NaN           1.0      NaN    30.0     NaN            NaN\n",
       "0.001737    NaN           1.0      NaN     4.0     NaN            NaN\n",
       "0.001763    NaN           1.0      NaN    87.0     NaN            NaN\n",
       "0.001789    NaN           1.0      NaN    17.0     NaN            NaN\n",
       "0.001804    NaN           2.0      NaN    45.0     NaN            1.0\n",
       "0.001852    NaN           1.0      NaN   229.0     NaN            NaN\n",
       "0.001864    NaN           1.0      NaN     NaN     NaN            NaN\n",
       "0.001903    NaN           1.0      NaN  1087.0     NaN            NaN\n",
       "0.001904    NaN           2.0      NaN    86.0     NaN            NaN\n",
       "0.001928    NaN           1.0      NaN    44.0     NaN            NaN\n",
       "0.001974    NaN           1.0      NaN    40.0     NaN            NaN\n",
       "0.002025    NaN           1.0      NaN    35.0     NaN            NaN\n",
       "0.002039    NaN           1.0      NaN    40.0     NaN            5.0\n",
       "0.002079    NaN           1.0      NaN     6.0     NaN            NaN\n",
       "0.002129    NaN           1.0      NaN    28.0     NaN            NaN\n",
       "0.002170    NaN           1.0      NaN     2.0     NaN            NaN\n",
       "0.002300    NaN           1.0      NaN     NaN     NaN            1.0\n",
       "\n",
       "[923 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= df_sub[initialcols].apply(lambda x: np.around(x,decimals=6).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T10:43:35.195510Z",
     "start_time": "2018-02-05T10:43:27.046257Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train, y['toxic'].values, test_size=0.33, random_state=42)\n",
    "\n",
    "toxic_model_preds = pd.DataFrame(y_test,columns=['true'],index=X_test.index)\n",
    "toxic_model_preds['toxic'] = 0\n",
    "\n",
    "xgb.XGBClassifier(n_estimators=100, subsample=0.8, colsample_bytree=0.8,  \n",
    "                  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27, \n",
    "                  gamma=0.1, learning_rate=0.1, max_depth= 3, min_child_weight= 3)\n",
    "\n",
    "kfold = 5\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=42)\n",
    "\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_valid = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "    y_train_fold, y_valid = y_train[train_index], y_train[test_index]\n",
    "    toxic_model.fit(X_train_fold,y_train_fold)\n",
    "    preds = toxic_model.predict_proba(X_test)[:,1]\n",
    "    toxic_model_preds['toxic'] += preds/kfold\n",
    "\n",
    "print('toxic_model_preds:\\n',log_loss(toxic_model_preds['true'],toxic_model_preds['toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T05:49:09.274728Z",
     "start_time": "2018-02-05T05:49:05.684181Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train, y['severe_toxic'].values, test_size=0.33, random_state=42)\n",
    "\n",
    "severe_toxic_model = xgb.XGBClassifier()\n",
    "severe_toxic_model.fit(X_train,y_train)\n",
    "severe_toxic_model_preds = pd.DataFrame([y_test,severe_toxic_model.predict_proba(X_test)[:,1]]).T.set_index(X_test.index)\n",
    "severe_toxic_model_preds.columns=['true','severe_toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T05:49:21.761462Z",
     "start_time": "2018-02-05T05:49:18.342167Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train, y['threat'].values, test_size=0.33, random_state=42)\n",
    "\n",
    "threat_model = xgb.XGBClassifier()\n",
    "threat_model.fit(X_train,y_train)\n",
    "threat_model_preds = pd.DataFrame([y_test,threat_model.predict_proba(X_test)[:,1]]).T.set_index(X_test.index)\n",
    "threat_model_preds.columns = ['true','threat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T05:49:34.735309Z",
     "start_time": "2018-02-05T05:49:31.164372Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train, y['obscene'].values, test_size=0.33, random_state=42)\n",
    "\n",
    "obscene_model = xgb.XGBClassifier()\n",
    "obscene_model.fit(X_train,y_train)\n",
    "obscene_model_preds = pd.DataFrame([y_test,obscene_model.predict_proba(X_test)[:,1]]).T.set_index(X_test.index)\n",
    "obscene_model_preds.columns=['true','obscene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T05:49:48.088742Z",
     "start_time": "2018-02-05T05:49:44.306002Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train, y['insult'].values, test_size=0.33, random_state=42)\n",
    "\n",
    "insult_model = xgb.XGBClassifier()\n",
    "insult_model.fit(X_train,y_train)\n",
    "insult_model_preds = pd.DataFrame([y_test,insult_model.predict_proba(X_test)[:,1]]).T.set_index(X_test.index)\n",
    "insult_model_preds = ['true','insult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T05:50:00.700834Z",
     "start_time": "2018-02-05T05:49:57.190543Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train, y['identity_hate'].values, test_size=0.33, random_state=42)\n",
    "\n",
    "identity_hate_model = xgb.XGBClassifier()\n",
    "identity_hate_model.fit(X_train,y_train)\n",
    "identity_hate_model_preds = pd.DataFrame([y_test,identity_hate_model.predict_proba(X_test)[:,1]]).T.set_index(X_test.index)\n",
    "identity_hate_model_preds.columns=['true','identity_hate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T11:07:18.847310Z",
     "start_time": "2018-02-05T11:07:18.753244Z"
    }
   },
   "outputs": [],
   "source": [
    "print('toxic_model_preds:\\n',log_loss(toxic_model_preds['true'],toxic_model_preds['toxic']))\n",
    "print('severe_toxic_model_preds:\\n',log_loss(severe_toxic_model_preds['true'],severe_toxic_model_preds['severe_toxic']))\n",
    "print('threat_model_preds:\\n',log_loss(threat_model_preds['true'],threat_model_preds['threat']))\n",
    "print('obscene_model_preds:\\n',log_loss(obscene_model_preds['true'],obscene_model_preds['obscene']))\n",
    "print('insult_model_preds:\\n',log_loss(insult_model_preds['true'],insult_model_preds['insult']))\n",
    "print('identity_hate_model_preds:\\n',log_loss(identity_hate_model_preds['true'],identity_hate_model_preds['identity_hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T23:50:31.563255Z",
     "start_time": "2018-02-01T23:49:00.361362Z"
    }
   },
   "outputs": [],
   "source": [
    "a = text_process(df_train_initial.comment_text[0:50000])\n",
    "b = text_process(df_train_initial.comment_text[50001:100000])\n",
    "aa = get_tags(df_train_initial[initialcols][0:50000])\n",
    "bb = get_tags(df_train_initial[initialcols][50001:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T23:54:19.289358Z",
     "start_time": "2018-02-01T23:50:32.728256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and test data. \n",
    "train_data = a\n",
    "train_labels = aa\n",
    "\n",
    "test_data = b\n",
    "test_labels = bb\n",
    "\n",
    "# Representation of the data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=10000, sublinear_tf=1,\n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(1,1))\n",
    "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
    "vectorised_test_data = vectorizer.transform(test_data)\n",
    "\n",
    "# Change the representation of our data as a list of bit lists \n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_train_labels = mlb.fit_transform(train_labels)\n",
    "binary_test_labels = mlb.transform(test_labels)\n",
    "\n",
    "print('Binary train labels:\\n', binary_train_labels)\n",
    "\n",
    "# One classifer built per category using a one vs the rest approach\n",
    "classifier = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "classifier.fit(vectorised_train_data, binary_train_labels)\n",
    "\n",
    "#Predict\n",
    "predictions = classifier.predict_proba(vectorised_test_data)\n",
    "\n",
    "\n",
    "print('Predictions:\\n', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T23:57:33.554200Z",
     "start_time": "2018-02-01T23:57:33.516176Z"
    }
   },
   "outputs": [],
   "source": [
    "# precision = precision_score(binary_test_labels, predictions)\n",
    "# recall = recall_score(binary_test_labels, predictions, average='micro')\n",
    "# f1 = f1_score(binary_test_labels, predictions, average='micro')\n",
    "# print(\"Micro-average quality numbers\")\n",
    "# print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision,\n",
    "#                                                                      recall,\n",
    "#                                                                      f1))\n",
    "\n",
    "precision = precision_score(binary_test_labels, predictions, average='macro')\n",
    "recall = recall_score(binary_test_labels, predictions, average='macro')\n",
    "f1 = f1_score(binary_test_labels, predictions, average='macro')\n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision,\n",
    "                                                                     recall,\n",
    "                                                                     f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T22:59:07.644626Z",
     "start_time": "2018-02-01T22:59:02.186015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and test data. \n",
    "train_data = a\n",
    "train_labels = aa\n",
    "\n",
    "test_data = b\n",
    "test_labels = bb\n",
    "\n",
    "# Representation of the data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=10000, sublinear_tf=1,\n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(1,1))\n",
    "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
    "vectorised_test_data = vectorizer.transform(test_data)\n",
    "\n",
    "# Change the representation of our data as a list of bit lists \n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_train_labels = mlb.fit_transform(train_labels)\n",
    "binary_test_labels = mlb.transform(test_labels)\n",
    "\n",
    "print('Binary train labels:\\n', binary_train_labels)\n",
    "\n",
    "# One classifer built per category using a one vs the rest approach\n",
    "classifier = OneVsRestClassifier(LinearSVC())\n",
    "classifier.fit(vectorised_train_data, binary_train_labels)\n",
    "\n",
    "#Predict\n",
    "predictions = classifier.predict(vectorised_test_data)\n",
    "\n",
    "\n",
    "print('Predictions:\\n', predictions)\n",
    "print()\n",
    "\n",
    "print('Predictions inverse:\\n', mlb.inverse_transform(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T22:59:09.010552Z",
     "start_time": "2018-02-01T22:59:08.885455Z"
    }
   },
   "outputs": [],
   "source": [
    "precision = precision_score(binary_test_labels, predictions, average='micro')\n",
    "recall = recall_score(binary_test_labels, predictions, average='micro')\n",
    "f1 = f1_score(binary_test_labels, predictions, average='micro')\n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision,\n",
    "                                                                     recall,\n",
    "                                                                     f1))\n",
    "\n",
    "precision = precision_score(binary_test_labels, predictions, average='macro')\n",
    "recall = recall_score(binary_test_labels, predictions, average='macro')\n",
    "f1 = f1_score(binary_test_labels, predictions, average='macro')\n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision,\n",
    "                                                                     recall,\n",
    "                                                                     f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T22:55:17.415443Z",
     "start_time": "2018-02-01T22:48:59.370Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions,columns=mlb.classes_+'pred').merge(df_train[50001:100000].reset_index(),\n",
    "                                                            left_index=True,right_index=True)\\\n",
    "                                                     .sort_values(by='total_flags',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
