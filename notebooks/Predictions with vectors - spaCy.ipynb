{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T02:50:02.984261Z",
     "start_time": "2018-02-13T02:50:02.328798Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import string\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import spacy\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T02:50:03.595736Z",
     "start_time": "2018-02-13T02:50:02.986265Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk import ngrams\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T02:50:09.271766Z",
     "start_time": "2018-02-13T02:50:03.596737Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_initial = pd.read_csv('train.csv.zip')\n",
    "df_test_initial = pd.read_csv('test.csv.zip')\n",
    "df_sub = pd.read_csv('sample_submission.csv.zip')\n",
    "\n",
    "initialcols = list(df_train_initial.columns[df_train_initial.dtypes == 'int64'])\n",
    "\n",
    "badwords_short = pd.read_csv('badwords_short.txt',header=None)\n",
    "badwords_short.rename(columns={0:'badwords_short'},inplace=True)\n",
    "badwords_short['badwords_short'] = badwords_short['badwords_short'].str.lower()\n",
    "badwords_short = badwords_short.drop_duplicates().reset_index(drop=True)\n",
    "badwords_short_set = set(badwords_short['badwords_short'].str.replace('*',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T02:50:29.015736Z",
     "start_time": "2018-02-13T02:50:09.278762Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T02:50:29.062769Z",
     "start_time": "2018-02-13T02:50:29.018739Z"
    }
   },
   "outputs": [],
   "source": [
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def get_lemma(message):\n",
    "    only_words = regexp_tokenizer.tokenize(message)\n",
    "    spacy_tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "    filtered_message = spacy_tokenizer(' '.join(only_words))\n",
    "    filtered_message_list = [t.lemma_ for t in filtered_message]\n",
    "    return filtered_message_list\n",
    "\n",
    "def get_shape(message):\n",
    "    only_words = regexp_tokenizer.tokenize(message)\n",
    "    spacy_tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "    filtered_message = spacy_tokenizer(' '.join(only_words))\n",
    "    filtered_message_list = list([t.shape_ for t in filtered_message])\n",
    "    return filtered_message_list\n",
    "\n",
    "def get_tag(message):\n",
    "    only_words = regexp_tokenizer.tokenize(message)\n",
    "    filtered_message = nlp(' '.join(only_words),disable=['textcat', 'parser', 'ner'])\n",
    "    filtered_message_list = list([str(t.tag_) for t in filtered_message])\n",
    "    return filtered_message_list\n",
    "\n",
    "def get_oov(message):\n",
    "    only_words = regexp_tokenizer.tokenize(message)\n",
    "    spacy_tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "    filtered_message = spacy_tokenizer(' '.join(only_words))\n",
    "    filtered_message_list = list([t.is_oov for t in filtered_message])\n",
    "    return filtered_message_list\n",
    "\n",
    "def get_ngram(message):\n",
    "    only_words = regexp_tokenizer.tokenize(message)\n",
    "    filtered_message = ' '.join(only_words)\n",
    "    filtered_message_list = ngrams(filtered_message.split(),3)\n",
    "    #filtered_message_list.extend(list(ngrams(filtered_message.split(),3)))\n",
    "    #filtered_message = [i for i in filtered_message if all(j.isnumeric()==False for j in i)]\n",
    "    return filtered_message_list\n",
    "\n",
    "def get_punct(message):\n",
    "    only_puncts = [i for i in message.split() if all(j in string.punctuation for j in i)]\n",
    "    return only_puncts\n",
    "\n",
    "model= {}\n",
    "y_train= {}\n",
    "y_test = {}\n",
    "preds={}\n",
    "preds_sub={}\n",
    "proc={}\n",
    "vec={}\n",
    "vec_test={}\n",
    "combined={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T02:50:29.105801Z",
     "start_time": "2018-02-13T02:50:29.064771Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(flags,test=True):\n",
    "     \n",
    "    if test==True:\n",
    "        for col in flags:\n",
    "            X_train, X_test, y_train[col], y_test[col] = train_test_split(df_train_initial.comment_text, \n",
    "                                                                          df_train_initial[col], \n",
    "                                                                          test_size=0.33, random_state=42)\n",
    "    else:\n",
    "        X_train = df_train_initial.comment_text.copy()\n",
    "        X_test = df_test_initial.comment_text.copy()\n",
    "        for col in flags:\n",
    "            y_train[col] = df_train_initial[col].copy()\n",
    "      \n",
    "    proc['lemma'] = TfidfVectorizer(tokenizer=get_lemma,min_df=5,strip_accents='unicode',sublinear_tf=1)\n",
    "    proc['punct']= TfidfVectorizer(tokenizer=get_punct,min_df=2,strip_accents='unicode',sublinear_tf=1)\n",
    "    proc['shape']= TfidfVectorizer(tokenizer=get_shape,min_df=5,strip_accents='unicode',sublinear_tf=1)\n",
    "    proc['tag']= TfidfVectorizer(tokenizer=get_tag,min_df=1,strip_accents='unicode',sublinear_tf=1)\n",
    "    proc['ngram']= TfidfVectorizer(tokenizer=get_ngram,min_df=2,strip_accents='unicode',sublinear_tf=1)\n",
    "    proc['oov']= TfidfVectorizer(tokenizer=get_oov,min_df=1,strip_accents='unicode',sublinear_tf=1)\n",
    "\n",
    "    for i in proc.keys():\n",
    "        vec[i] = proc[i].fit_transform(X_train)\n",
    "        print('Fit vec:',i)\n",
    "        vec_test[i] = proc[i].transform(X_test)\n",
    "        print('Fit vec_test:',i)\n",
    "\n",
    "\n",
    "    combined['train'] = hstack([vec[i] for i in vec])\n",
    "    combined['test'] = hstack([vec_test[i] for i in vec_test])\n",
    "\n",
    "    for col in flags:     \n",
    "        \n",
    "        model[col] = LogisticRegression(solver='sag',C=3,max_iter=250,n_jobs=-1)\n",
    "        model[col].fit(combined['train'],y_train[col].tolist())\n",
    "        \n",
    "        if test==True:\n",
    "            preds[col] = model[col].predict_proba(combined['test'])[:,1]\n",
    "            print(col,'model predictions:\\n',roc_auc_score(y_test[col],preds[col]))\n",
    "        else:\n",
    "            preds_sub[col] = model[col].predict_proba(combined['test'])[:,1]\n",
    "            df_sub[col] = preds_sub[col]\n",
    "            print(col,'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T02:50:30.730953Z",
     "start_time": "2018-02-13T02:50:29.107802Z"
    }
   },
   "outputs": [],
   "source": [
    "vec['lemma'] = pickle.load(open('lemma_vector.sav', 'rb'))\n",
    "vec['punct']= pickle.load(open('punct_vector.sav', 'rb'))\n",
    "vec['shape']= pickle.load(open('shape_vector.sav', 'rb'))\n",
    "vec['tag']= pickle.load(open('tag_vector.sav', 'rb'))\n",
    "vec['ngram']= pickle.load(open('ngram_vector.sav', 'rb'))\n",
    "vec['oov']= pickle.load(open('oov_vector.sav', 'rb'))\n",
    "\n",
    "vec_test['lemma'] = pickle.load(open('lemma_test_vector.sav', 'rb'))\n",
    "vec_test['punct']= pickle.load(open('punct_test_vector.sav', 'rb'))\n",
    "vec_test['shape']= pickle.load(open('shape_test_vector.sav', 'rb'))\n",
    "vec_test['tag']= pickle.load(open('tag_test_vector.sav', 'rb'))\n",
    "vec_test['ngram']= pickle.load(open('ngram_test_vector.sav', 'rb'))\n",
    "vec_test['oov']= pickle.load(open('oov_test_vector.sav', 'rb'))\n",
    "\n",
    "combined['train'] = hstack([vec[i] for i in vec])\n",
    "combined['test'] = hstack([vec_test[i] for i in vec_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:14:54.581753Z",
     "start_time": "2018-02-13T03:14:46.173163Z"
    }
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "\n",
    "allresp={}\n",
    "j=0\n",
    "\n",
    "API_KEY='AIzaSyCnZK0yYz6ml1f_eWSaOrjY3Wt9AZUEtgs'\n",
    "\n",
    "# Generates API client object dynamically based on service name and version.\n",
    "service = discovery.build('commentanalyzer', 'v1alpha1', developerKey=API_KEY)\n",
    "\n",
    "for i in df_train_initial.comment_text[0:10]:\n",
    "    \n",
    "    analyze_request = {\n",
    "      'comment': { 'text': i },\n",
    "      'requestedAttributes': {'TOXICITY': {},\n",
    "                              'OBSCENE': {},\n",
    "                              'ATTACK_ON_AUTHOR': {},\n",
    "                              'ATTACK_ON_COMMENTER': {},\n",
    "                              'SEVERE_TOXICITY': {}}\n",
    "    }\n",
    "    response = service.comments().analyze(body=analyze_request).execute()\n",
    "    allresp[j]={}\n",
    "    for k in response['attributeScores']:\n",
    "        allresp[j][k] = response['attributeScores'][k]['summaryScore']['value']\n",
    "    j+=1\n",
    "    \n",
    "allresp\n",
    "\n",
    "# import json\n",
    "# print (json.dumps(response, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T06:47:34.686608Z",
     "start_time": "2018-02-12T06:43:00.753374Z"
    }
   },
   "outputs": [],
   "source": [
    "modellrSFM={}\n",
    "sfm={}\n",
    "sfm_train={}\n",
    "sfm_test={}\n",
    "for col in initialcols:\n",
    "    modellrSFM[col] = SelectFromModel(model[col],prefit=True,threshold='6*mean')\n",
    "    sfm_train[col] = modellrSFM[col].transform(combined['train'])\n",
    "    sfm_test[col] = modellrSFM[col].transform(combined['test'])\n",
    "    sfm[col] = LogisticRegression(solver='sag',C=3,max_iter=250,n_jobs=-1)\n",
    "    sfm[col].fit(sfm_train[col],y_train[col].tolist())\n",
    "    print(col,'fit')\n",
    "    preds_sub[col] = sfm[col].predict_proba(sfm_test[col])[:,1]\n",
    "    print(col,'predict')\n",
    "    df_sub[col] = preds_sub[col]\n",
    "    \n",
    "df_sub.to_csv('df_sub_'+datetime.datetime.now().strftime('%Y%m%d%I%M')+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:50:43.255825Z",
     "start_time": "2018-02-12T03:34:33.815910Z"
    }
   },
   "outputs": [],
   "source": [
    "modelxgb= {}\n",
    "preds_sub={}\n",
    "for col in initialcols:     \n",
    "\n",
    "    modelxgb[col] = xgb.XGBClassifier(n_estimators=300, max_depth=5,objective= 'binary:logistic')\n",
    "    modelxgb[col].fit(combined['train'],y_train[col].tolist())\n",
    "\n",
    "    preds_sub[col] = modelxgb[col].predict_proba(combined['test'])[:,1]\n",
    "    df_sub[col] = preds_sub[col]\n",
    "    print(col,'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:27:14.856504Z",
     "start_time": "2018-02-12T02:37:06.130026Z"
    }
   },
   "outputs": [],
   "source": [
    "make_model(initialcols,test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:27:16.784237Z",
     "start_time": "2018-02-12T03:27:15.125265Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in vec.keys():\n",
    "    pickle.dump(vec[i], open(i+'_vector.sav', 'wb'))\n",
    "    pickle.dump(vec_test[i], open(i+'_test_vector.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T01:10:26.454865Z",
     "start_time": "2018-02-13T01:10:20.753773Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv('df_sub_'+datetime.datetime.now().strftime('%Y%m%d%I%M')+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:28:06.063253Z",
     "start_time": "2018-02-12T03:28:06.023225Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "683px",
    "left": "1544px",
    "right": "20px",
    "top": "132px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
