{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T23:24:35.539161Z",
     "start_time": "2018-02-08T23:24:31.804869Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import string\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T23:24:39.818264Z",
     "start_time": "2018-02-08T23:24:35.541081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk import ngrams\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T23:24:43.069013Z",
     "start_time": "2018-02-08T23:24:39.822263Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_initial = pd.read_csv('train.csv.zip')\n",
    "df_test_initial = pd.read_csv('test.csv.zip')\n",
    "df_sub = pd.read_csv('sample_submission.csv.zip')\n",
    "\n",
    "initialcols = list(df_train_initial.columns[df_train_initial.dtypes == 'int64'])\n",
    "\n",
    "badwords_short = pd.read_csv('badwords_short.txt',header=None)\n",
    "badwords_short.rename(columns={0:'badwords_short'},inplace=True)\n",
    "badwords_short['badwords_short'] = badwords_short['badwords_short'].str.lower()\n",
    "badwords_short = badwords_short.drop_duplicates().reset_index(drop=True)\n",
    "badwords_short_set = set(badwords_short['badwords_short'].str.replace('*',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T23:24:43.138049Z",
     "start_time": "2018-02-08T23:24:43.075004Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def get_ngrams(message):\n",
    "    only_words = tokenizer.tokenize(message)\n",
    "    filtered_message = ' '.join(only_words)\n",
    "    filtered_message_list = list(ngrams(filtered_message.split(),2))\n",
    "    filtered_message_list.extend(list(ngrams(filtered_message.split(),3)))\n",
    "    #filtered_message = [i for i in filtered_message if all(j.isnumeric()==False for j in i)]\n",
    "    return filtered_message_list\n",
    "\n",
    "def get_words(message):\n",
    "    only_words = tokenizer.tokenize(message)\n",
    "    return only_words\n",
    "\n",
    "def get_puncts(message):\n",
    "    only_puncts = [i for i in message.split() if all(j in string.punctuation for j in i)]\n",
    "    return only_puncts\n",
    "\n",
    "def get_badwords(message):\n",
    "    only_bad=[]\n",
    "    for word in badwords_short_set:\n",
    "        count = message.lower().count(word)\n",
    "        if count>0:\n",
    "            for i in range(0,count):\n",
    "                only_bad.append('found_in_badwords_short_'+word)\n",
    "    return only_bad \n",
    "\n",
    "model= {}\n",
    "y_train= {}\n",
    "y_test = {}\n",
    "preds={}\n",
    "preds_sub={}\n",
    "proc={}\n",
    "vec={}\n",
    "vec_test={}\n",
    "combined={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T23:24:43.217006Z",
     "start_time": "2018-02-08T23:24:43.141056Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(flags,test=True):\n",
    "     \n",
    "    if test==True:\n",
    "        for col in flags:\n",
    "            X_train, X_test, y_train[col], y_test[col] = train_test_split(df_train_initial.comment_text, \n",
    "                                                                          df_train_initial[col], \n",
    "                                                                          test_size=0.33, random_state=42)\n",
    "    else:\n",
    "        X_train = df_train_initial.comment_text.copy()\n",
    "        X_test = df_test_initial.comment_text.copy()\n",
    "        for col in flags:\n",
    "            y_train[col] = df_train_initial[col].copy()\n",
    "      \n",
    "    proc['words'] = TfidfVectorizer(analyzer=get_words,min_df=3,strip_accents='unicode',sublinear_tf=1)\n",
    "    proc['puncts']= TfidfVectorizer(analyzer=get_puncts,min_df=2,strip_accents='unicode',sublinear_tf=1)\n",
    "    proc['ngrams']= TfidfVectorizer(analyzer=get_ngrams,min_df=4,strip_accents='unicode',sublinear_tf=1)\n",
    "    proc['badwords']= TfidfVectorizer(analyzer=get_badwords,min_df=1,strip_accents='unicode',sublinear_tf=1)\n",
    "\n",
    "    vec['words'] = proc['words'].fit_transform(X_train)\n",
    "    vec['puncts'] = proc['puncts'].fit_transform(X_train)\n",
    "    vec['ngrams'] = proc['ngrams'].fit_transform(X_train)\n",
    "    vec['badwords'] = proc['badwords'].fit_transform(X_train)\n",
    "\n",
    "    vec_test['words']=proc['words'].transform(X_test)\n",
    "    vec_test['puncts']=proc['puncts'].transform(X_test)\n",
    "    vec_test['ngrams']=proc['ngrams'].transform(X_test)\n",
    "    vec_test['badwords']=proc['badwords'].transform(X_test)\n",
    "        \n",
    "    combined['train'] = hstack([vec['words'],vec['puncts'],vec['ngrams'],vec['badwords']])\n",
    "    combined['test'] = hstack([vec_test['words'],vec_test['puncts'],vec_test['ngrams'],vec_test['badwords']])\n",
    "    \n",
    "    for col in flags:     \n",
    "        model[col]={}\n",
    "        \n",
    "        model[col]['lr'] = LogisticRegression(solver='sag',C=3,max_iter=200,n_jobs=-1)\n",
    "        model[col]['lr'].fit(combined['train'],y_train[col].tolist())\n",
    "        \n",
    "        model[col]['xgb'] = xgb.XGBClassifier(n_estimators=300, max_depth=5,objective= 'binary:logistic', \n",
    "                                              scale_pos_weight=1, seed=27, base_score = .2)\n",
    "        model[col]['xgb'].fit(combined['train'],y_train[col].tolist(),eval_metric='auc')\n",
    "        \n",
    "        model[col]['gbc'] = GradientBoostingClassifier()\n",
    "        model[col]['gbc'].fit(combined['train'],y_train[col].tolist())\n",
    "        \n",
    "\n",
    "        if test==True:\n",
    "            preds[col]={}\n",
    "            for i in model[col].keys():\n",
    "                preds[col][i] = model[col][i].predict_proba(combined['test'])[:,1]\n",
    "                print(col,i,'model predictions:\\n',roc_auc_score(y_test[col],preds[col][i]))\n",
    "                allpreds+=preds[col][i]\n",
    "            allpreds/=3\n",
    "            print(col,'model predictions:\\n',roc_auc_score(y_test[col],allpreds))\n",
    "        else:\n",
    "            preds_sub[col]={}\n",
    "            allpreds=np.zeros(combined['test'].shape[0])\n",
    "            for i in model[col].keys():\n",
    "                preds_sub[col][i] = model[col][i].predict_proba(combined['test'])[:,1]\n",
    "                allpreds+=preds_sub[col][i]\n",
    "            allpreds/=3\n",
    "            df_sub[col] = allpreds\n",
    "            print(col,'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T01:41:09.408053Z",
     "start_time": "2018-02-08T23:24:43.217991Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic done\n",
      "severe_toxic done\n",
      "obscene done\n",
      "threat done\n",
      "insult done\n",
      "identity_hate done\n"
     ]
    }
   ],
   "source": [
    "make_model(initialcols,test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T02:03:04.013196Z",
     "start_time": "2018-02-09T02:03:03.994174Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub['toxic'] = preds_sub['toxic']['lr']\n",
    "df_sub['severe_toxic'] = preds_sub['severe_toxic']['lr']\n",
    "df_sub['obscene'] = preds_sub['obscene']['lr']\n",
    "df_sub['threat'] = preds_sub['threat']['lr']\n",
    "df_sub['insult'] = preds_sub['insult']['lr']\n",
    "df_sub['identity_hate'] = preds_sub['identity_hate']['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T06:19:07.853514Z",
     "start_time": "2018-02-09T06:19:05.483344Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "for i in vec.keys():\n",
    "    pickle.dump(vec[i], open(i+'_vector.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T02:03:10.261781Z",
     "start_time": "2018-02-09T02:03:08.277639Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv('df_sub_'+datetime.datetime.now().strftime('%Y%m%d%I%M')+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T21:00:35.444709Z",
     "start_time": "2018-02-08T12:11:24.327317Z"
    }
   },
   "outputs": [],
   "source": [
    "# C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
    "# toxic lr model predictions:\n",
    "#  0.973623915807\n",
    "# toxic xgb model predictions:\n",
    "#  0.957367570947\n",
    "# toxic gbc model predictions:\n",
    "#  0.920677283411\n",
    "# toxic model predictions:\n",
    "#  0.967328623644\n",
    "# severe_toxic lr model predictions:\n",
    "#  0.988066880563\n",
    "# severe_toxic xgb model predictions:\n",
    "#  0.981223988455\n",
    "# severe_toxic gbc model predictions:\n",
    "#  0.946132712332\n",
    "# severe_toxic model predictions:\n",
    "#  0.987947888331\n",
    "# obscene lr model predictions:\n",
    "#  0.98715018023\n",
    "# obscene xgb model predictions:\n",
    "#  0.983366581819\n",
    "# obscene gbc model predictions:\n",
    "#  0.966495202699\n",
    "# obscene model predictions:\n",
    "#  0.987547215406\n",
    "# threat lr model predictions:\n",
    "#  0.984074679767\n",
    "# threat xgb model predictions:\n",
    "#  0.965280067921\n",
    "# threat gbc model predictions:\n",
    "#  0.542049593889\n",
    "# threat model predictions:\n",
    "#  0.983671224789\n",
    "# insult lr model predictions:\n",
    "#  0.98025063686\n",
    "# insult xgb model predictions:\n",
    "#  0.972816999733\n",
    "# insult gbc model predictions:\n",
    "#  0.953063786142\n",
    "# insult model predictions:\n",
    "#  0.978857091119\n",
    "# identity_hate lr model predictions:\n",
    "#  0.977883100898\n",
    "# identity_hate xgb model predictions:\n",
    "#  0.970471305196\n",
    "# identity_hate gbc model predictions:\n",
    "#  0.876133030069\n",
    "# identity_hate model predictions:\n",
    "#  0.979015052878"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "683px",
    "left": "1544px",
    "right": "20px",
    "top": "132px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
